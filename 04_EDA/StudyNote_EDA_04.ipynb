{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54b50bc9",
   "metadata": {},
   "source": [
    "# 웹 데이터 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db12227c",
   "metadata": {},
   "source": [
    "```\n",
    "cd DS_study/ds_study/WebData\n",
    "conda activate ds_study\n",
    "code .\n",
    "```\n",
    "ctrl + shfit + P"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4c8470",
   "metadata": {},
   "source": [
    "## Beautiful Soup\n",
    "- tag로 이루어진 문서를 해석하는 python 라이브러리\n",
    "- `from ba4 import BeautifulSoup`\n",
    "\t- open()\n",
    "\t\t- 파일명과 함께 읽기(r) / 쓰기(w) 속성 지정\n",
    "\t\t- `page = open(\"파일주소.html\", 'r').read()`\n",
    "\t- html.parser\n",
    "\t\t- Beutiful Soup의 html을 읽는 엔진 중 하나\n",
    "\t\t- `soup = BeautifulSoup(page, \"html.parser')`\n",
    "\t\t\t- 들여쓰기가 잘 안되어있고, 읽기 불편함\n",
    "\t- prettify()\n",
    "\t\t- html 출력, 들여쓰기를 이쁘게 만들어 주는 기능\n",
    "\t\t- `print(soup.prettify())`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1dc33a6",
   "metadata": {},
   "source": [
    "### html 기초\n",
    "- 03. test_first.html 파일 활용하여 실습 진행\n",
    "- html 언어\n",
    "\t- 브라우저를 통해 예쁜 화면을 제공함\n",
    "\t- tag로 이루어져 있음\n",
    "\t\t- html\n",
    "\t\t\t- 웹 페이지를 표현\n",
    "\t\t- head\n",
    "\t\t\t- 눈에 보이지 않지만 문서에 필요한 헤더 정보 보관\n",
    "\t\t- body\n",
    "\t\t\t- 눈에 보이는 정보를 보관\n",
    "\t\t\t- div\n",
    "\t\t\t\t- \n",
    "\t\t\t- p\n",
    "\t\t\t\t- 문단\n",
    "\t\t\t- class\n",
    "\t\t\t\t- 구분 속성\n",
    "\t\t\t- id\n",
    "\t\t\t\t- 구분 속성\n",
    "\t\t\t- a\n",
    "\t\t\t\t- \n",
    "\t\t\t- href\n",
    "\t\t\t\t- 주소창\n",
    "\t\t\t- target\n",
    "\t\t\t\t- `_blink`\n",
    "\t\t\t\t\t- 새창\n",
    "\t\t\t- b\n",
    "\t\t\t\t- 굵게\n",
    "\t\t\t- i\n",
    "\t\t\t\t- 기울여서\n",
    "- 특정 태그가 보고 싶을 때\n",
    "\t- `soup.body`\n",
    "\t- `soup['body']`\n",
    "\t- `soup.find('p')`\n",
    "\t\t- 1개만 찾아줌\n",
    "\t- `soup.find_all('p')`\n",
    "\t\t- 지정된 태그 모두를 찾아 리스트 형태로 반환\n",
    "\t- 조건을 좁히는 방법으로 class, id를 태그와 함께 추가할 수 있다\n",
    "\t\t- `soup.find_all(class_='클래스명')`\n",
    "\t\t- `soup.find_all({'class' : '클래스명'})`\n",
    "\t\t\t- 특정 클래스를 찾는 방법\n",
    "\t\t- `soup.find_all(id_='아이디명')`\n",
    "\t\t\t- 특정 id로 찾는 방법\n",
    "\t- 담아져 있는 내용 출력\n",
    "\t\t- `soup.find('p').text`\n",
    "\t\t\t- 출력문자가 나옴\n",
    "\t\t\t- 공백을 지우기 위해 끝에 `.strip()`을 붙이면 됨\n",
    "\t\t- `soup.find('p').string`\n",
    "\t\t- `soup.find_all('p').get_test()`\n",
    "\t\t\t- p로 쌓여있는 실제 출력 글자 가져오는 함수\n",
    "\t\t- `soup.find_all('a')[0].get('href')`\n",
    "\t\t\t- a태그의 첫 번째 요소의 주소값을 가져오기\n",
    "- a 태그\n",
    "\t- 링크\n",
    "\t- `<a href = \"주소\" id = \"아이디값\">출력글자</a>`\n",
    "\t\t- `soup.find_all('a').['href'].string`\n",
    "\t\t\t- a 태그의 href 속성의 글자를 가져오는 방법\n",
    "\t\t\t- 출력 글자가 나옴"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c4dab2",
   "metadata": {},
   "source": [
    "## 크롬 개발자 도구\n",
    "- 웹데이터 검출하고 싶은데 html을 잘 모를 때 활용하는 도구\n",
    "- 크롬 설정 > 도구 더보기 > 개발자 도구\n",
    "\t- 필요 데이터 부분 선택하면, 어느 부분인지 태그가 보임\n",
    "\t- `<span class='value>`를 기억해야 한다\n",
    "- 웹 주소에 접근할 때 필요한 라이브러리\n",
    "\t- `from urllib.request import urlopen`\n",
    "\t- `urlopen('url주소')`를 beautifulsoup으로 해석하기\n",
    "\t- `ulropen('url주소').status`\n",
    "\t\t- 응답 상황을 알 수 있다, http 상태코드 반환\n",
    "\t\t- 200\n",
    "\t\t\t- 정상\n",
    "- 프롬프트에서 특정 라이브러리 잘 인스톨되었는지 확인\n",
    "\t- `pip list | findstr 라이브러리명`\n",
    "- **requests 라이브러리 깔았는데 임포트되지 않아 오류 내용을 보니 chardet이 없다고 해서 깔고 마저 하니 되었다.** \n",
    "- requests 라이브러리와 urllib.requests.Request는 비슷한 역할을 한다\n",
    "\t- `requests.get()`\n",
    "\t- `requests.post()`\n",
    "- 무언가를 찾는 함수\n",
    "\t- `find` 와 비슷한 것 `select one`\n",
    "\t- `find_all` 와 비슷한 것 `select`\n",
    "\t- select, select one 의 문법\n",
    "\t\t- `~~.select(\"#아이디명 > li\")`\n",
    "\t\t- class면 . 을 활용\n",
    "\t\t- id 면 # 을 활용\n",
    "\t\t- 만약 띄어쓰기가 존재하면 속성이 여러 개, .으로 이어주기\n",
    "\t\t- 상, 하위 이동이 간결함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52137f23",
   "metadata": {},
   "source": [
    "## 위키백과 문서 정보 가져오기\n",
    "- 한글로만 이루어진 페이지, 여명의 눈동자 위키백과 웹페이지 활용\n",
    "- 한글로 되어있는 웹페이지 주소는 옮길 경우 문자의 나열이 바뀐다.\n",
    "```\n",
    "import urllib\n",
    "from urllib.request import urlopen, Request\n",
    "\n",
    "html = 'http://ko.wikipedia.org/wiki/{search_words}'\n",
    "req = Request(html.format(search_words = urllib.parse.quote('여명의_눈동자'))) # 글자를 URL로 인코딩\n",
    "\n",
    "response = urlopen(req)\n",
    "\n",
    "soup = BeautifulSoup(response, 'html.parser')\n",
    "\n",
    "soup.find_all('ul')[15],text.strip().replace('\\xa0', '').replace('\\n', '')\n",
    "# 바꾸고 싶은 문자를 찾아 replace함수에 넣고\n",
    "# 찾고 싶은 부분의 순서를 찾아 15 자리에 넣는다.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4892e99",
   "metadata": {},
   "source": [
    "### list 데이터형\n",
    "- 리스트 변수를 선언할 때 생긴 메모리를 가리키는 주소가 복사됨\n",
    "- 반복문, 조건문에 적용하는 것이 편함\n",
    "- 리스트 내부에 리스트를 가질 수 있다\n",
    "- `isinstance(변수명, list)`\n",
    "\t- 자료형이 list인지 확인 가능\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f525a9a2",
   "metadata": {},
   "source": [
    "## 시카고 맛집 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70618375",
   "metadata": {},
   "source": [
    "### 시카고 맛집 메인페이지 분석\n",
    "```\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from urllib.request import Request, urlopen\n",
    "\n",
    "# 메인페이지와 서브페이지 분석을 따로 해야할 수 있기 때문에 주소를 두 개로 분리\n",
    "url_base = 'https://www.chicagomag.com'\n",
    "url_sub = '/Chicago-Magazine/November-2012/Best-Sandwiches-Chicago/'\n",
    "url = url_base + url_sub\n",
    "\n",
    "# http 403은 서버에서 유저가 문제가 있다고 하는 것\n",
    "# \n",
    "req = Request(url, headers = {'User-Agent': 'Chrome'})\n",
    "html = urlopen(req).read()\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "soup.prettify()\n",
    "\n",
    "soup.find_all('div', _class = 'sammy'), len(soup.find_all('div', class_ = 'sammy')) # 갯수확인\n",
    "# select로도 가능\n",
    "soup.select(\".sammy\"), len(soup.select(\".sammy\"))\n",
    "\n",
    "# 전체 코드 긁어오기 위한 샘플 코드 테스트 중...\n",
    "tmp_one = soup.find_all('div', 'sammy')[0]\n",
    "type(tmp_one)\n",
    "# 위 값이 bs4.element.Tag 이면 bs4 요소이므로 bs4 함수를 적용할 수 있다\n",
    "\n",
    "tmp_one.find(class_ = 'sammyRank').get_text()\n",
    "tmp_one.find('div', {'class' : 'sammyRank'}).get_text()\n",
    "tmp_one.select_one('.sammylisiting').text()\n",
    "tmp_one.find(\"a\")['href']\n",
    "tmp_one.select_one(\"a\").get('href')\n",
    "```\n",
    "- 크롬 개발자 도구 내 network 탭을 살펴보면 요청과 그에 상응하는 응답이 기록된다\n",
    "\t- Request Headers 토글을 열어보면 user-agent 정보가 있다. 그곳에 나온 정보를 활용하는게 정석이나, 간략하게 Chrome 정도로 사용해도 좋다.\n",
    "\t- req.status 를 보면 200은 응답을 제대로 받았다, 403번은 별로인 상태\n",
    "- fake-useragent 라이브러리 \n",
    "\t- from fake_useragent import UserAgent\n",
    "\t- UserAgent().ie\n",
    "\t\t- user-agent에 들어갈 값을 랜덤으로 만들어준다\n",
    "- type(찾은 요소) = bs4.element.Tag 이면, find 명령을 또 활용할 수 있다는 의미\n",
    "- 연결되는 홈페이지 주소가 상대경로이기 때문에, 주소를 상위 페이지와 하위 페이지로 구분해 분석하는 것이 용이하다\n",
    "- re 모듈\n",
    "```\n",
    "import re\n",
    "\n",
    "# \\n 혹은 \\r\\n이 보이면 분리해라\n",
    "re.split(()'\\n|\\r\\n'), 문자열)\n",
    "```\n",
    "\n",
    "```\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "url_base = 'https://www.chicagomag.com'\n",
    "\n",
    "rank = []\n",
    "main_menu = []\n",
    "cafe_name []\n",
    "url_add = []\n",
    "\n",
    "list_soup = soup.find_all('div', 'sammy')\n",
    "\n",
    "for item in list_soup:\n",
    "\trank.append(item.find(class_= 'sammyRank').get_text())\n",
    "\ttmp_string = item.find(class_= 'sammyListing').get_text()\n",
    "\tmain_menu.append(re.split(('\\n|\\r\\n'), tmp_string)[0])\n",
    "\tcafe_menu.append(re.split(('\\n|\\r\\n'), tmp_string)[1])\n",
    "\turl_add.append(urljoin(url_base, item.find('a')['href']))\n",
    "\t# 두 번째 인자(주소)가 절대 주소라면 그냥 사용하고, 상대 주소라면 첫 번째 인자(주소)를 붙여서 사용하라\n",
    "\n",
    "# 확인용\n",
    "len(rank), len(main_manu), len(cafe_name), len(url_add)\n",
    "rank[:5], ...\n",
    "\n",
    "# 데이터 정리\n",
    "\n",
    "data = {\n",
    "\t\"Rank\": rank,\n",
    "\t\"Menu\": main_menu,\n",
    "\t\"Cafe\": cafe_name,\n",
    "\t\"URL\": url_add\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df = pd.Dataframe(data, columns = ['Rank', 'Cafe', 'Menu', 'URL])\n",
    "df.to_csv(\"주소/이름.csv\", sep = \",\", encoding = 'utf-8')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cee9cf",
   "metadata": {},
   "source": [
    "### 시카고 맛집 하위 페이지 분석\n",
    "-  Regular Expession_기초\n",
    "\t- .x\n",
    "\t\t- 임의의 한 문자를 표현\n",
    "\t- x+\n",
    "\t\t- x가 1번 이상 반복\n",
    "\t- x?\n",
    "\t\t- x가 존재하거나 존재하지 않는다\n",
    "\t- x*\n",
    "\t\t- x가 0번 이상 반복\n",
    "\t- x|y\n",
    "\t\t- x또는 y를 찾는다\n",
    "```\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('주소/이.csv', index_col = 0)\n",
    "\n",
    "df['URL'][0]\n",
    "\n",
    "req = Request(df[\"URL\"][0], headers = {'User-Agent': 'Chrome'})\n",
    "html = urlopen(req).read()\n",
    "\n",
    "soup_tmp = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "print(soup_tmp.find('p', 'addy'))\n",
    "# 가격과 주소가 하나의 태그에 들어있다...!\n",
    "\n",
    "price_tmp = soup_tmp.find('p', 'addy').get_text()\n",
    "\n",
    "price_tmp = re.split(\".,\", price_tmp)[0]\n",
    "\n",
    "# 끝에 .group()을 해주어야 값만 나온다\n",
    "tmp = re.search(\"\\$\\d+\\.(\\d+)?\", price_tmp).group()\n",
    "# $가 반드시 있어야 하고, 숫자가 여러 개 있을 수 있고, .이 반드시 와야 하고, (숫자 여러개)의 것이 있을 수도 있고, 없을 수도 있다.\n",
    "\n",
    "# 가격과 띄어쓰기 포함한 것의 길이 다음은 주소가 나타남\n",
    "price_tmp[len(tmp) + 2:]\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "price = []\n",
    "address = []\n",
    "\n",
    "# 코드 작동 유무 확인 위해 세 번만 돌리기\n",
    "for n in df.index[:3]:\n",
    "\t# html = urlopen(df['URL'][n])\n",
    "\t\n",
    "\treq = request(df['URL'][n], headers = {\"User-Agent\": \"Mozilla/5.0\"})\n",
    "\thtml = urlopen(req).read()\n",
    "\t\n",
    "\tsoup_tmp = BeautifulSoup(html, 'lxml')\n",
    "\t\n",
    "\tgettings = soup_tml.find('p', 'addy').get_text()\n",
    "\t\n",
    "\tprice_tmp = re.split('.,', gettings)[0]\n",
    "\ttmp = re.search(\"\\$\\d+\\.(\\d+)?\", price_tmp).group()\n",
    "\t\n",
    "\tprice.append(tmp)\n",
    "\taddress.append(price_tmp[len(tmp) + 2 :])\n",
    "\t\n",
    "\tprint(n)\n",
    " \n",
    "```\n",
    "- python스러운 문법\n",
    "\t- list를 고대로 사용하기\n",
    "\t\t- 여러 컬럼을 for문 안에서 사용하기 어려움\n",
    "\t- iterrows() 함수를 활용\n",
    "```\n",
    "for idx, row in df.iterrows():\n",
    "\tprint(row['URL'])\n",
    "```\n",
    "\n",
    "- TQDM\n",
    "\t- 라이브러리\n",
    "\t- 코드 실행 중에 프로그래스 바가 보여짐\n",
    "\t- 오류 없이 제대로 모두 적용되었는지 확인하는데 활용하는 것\n",
    "```\n",
    "from tqdm import tqdm\n",
    "price = []\n",
    "address = []\n",
    "\n",
    "# 코드 작동 유무 확인 위해 세 번만 돌리기\n",
    "for idx, row in tqdm(df.index[:3].iterrows()):\n",
    "\t# html = urlopen(df['URL'][n])\n",
    "\t\n",
    "\treq = request(row['URL'], headers = {\"User-Agent\": \"Mozilla/5.0\"})\n",
    "\thtml = urlopen(req).read()\n",
    "\t# 홈페이지 수정중에 html parser로 불가능할 수 있음\n",
    "\t# conda 환경 사용 중에 lxml 환경이 없는 경우가 있다. 깔아주면 됨\n",
    "\tsoup_tmp = BeautifulSoup(html, 'lxml')\n",
    "\t\n",
    "\tgettings = soup_tml.find('p', 'addy').get_text()\n",
    "\t\n",
    "\tprice_tmp = re.split('.,', gettings)[0]\n",
    "\ttmp = re.search(\"\\$\\d+\\.(\\d+)?\", price_tmp).group()\n",
    "\t\n",
    "\tprice.append(tmp)\n",
    "\taddress.append(price_tmp[len(tmp) + 2 :])\n",
    "\t\n",
    "\tprint(n)\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "df['Price'] = price\n",
    "df['Address'] = address\n",
    "\n",
    "# price, address를 위해 link 데이터가 필요했던 것이므로 최종적으로는 삭제\n",
    "df = df.loc[:, ['Rank', 'Cafe', 'Menu', 'Price', 'Address']]\n",
    "df.set_index('Rank', inplace = True)\n",
    "\n",
    "df.to_csv('', sep = ',', encoding = 'utf-8')\n",
    "\n",
    "pd.read_csv('', index_col = 0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3320bd",
   "metadata": {},
   "source": [
    "### 시카고 맛집 데이터 지도 시각화\n",
    "```\n",
    "import folium\n",
    "import pandas as pd\n",
    "import googlemaps\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 주소 체크\n",
    "df_ pd.read_csv('', index_col = 0)\n",
    "\n",
    "gmaps_key = \"geocoding api key geocoding api key\"\n",
    "gmaps = googlemaps.Client(key = gmaps_key)\n",
    "\n",
    "lat = []\n",
    "lng = []\n",
    "\n",
    "for idx, row in tqdm(df.iterrows()):\n",
    "\tif not row[\"Address\"] == \"Multiple location\":\n",
    "\t\ttarget_name = row[\"Address\"] + \", \" + \"Chicago\"\n",
    "\t\tgmaps_output = gmaps.geocode(target_name)\n",
    "\t\tlocation_output = gmaps_output[0].get('geometry')\n",
    "\t\tlat.append(location_output[\"location\"][\"lat\"])\n",
    "\t\tlng.append(location_output[\"location\"][\"lng\"])\n",
    "\t\n",
    "\telse:\n",
    "\t\tlat.append(np.nan)\n",
    "\t\tlng.append(np.nan)\n",
    "\n",
    "df[\"lat\"] = lat\n",
    "df[\"lng\"] = lng\n",
    "\n",
    "df.head()\n",
    "\n",
    "mapping = folium.Map(location = [41.8781136, -87.6297982], zoom_start = 11)\n",
    "\n",
    "mapping\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "\tif not row[\"Address\"] == \"Multiple location\":\n",
    "\t\tfolium.Marker([row[\"lat\"],\n",
    "\t\trow[\"lng\"]],\n",
    "\t\tpopup = row[\"Cafe\"],\n",
    "\t\ttooltip = row[\"Menu\"],\n",
    "\t\ticon = folium.Icon(\n",
    "\t\t\tIcon = \"coffee\",\n",
    "\t\t\tprefix = \"fa\"\n",
    "\t\t\t)\n",
    "\t\t).add_to(mapping)\n",
    "\n",
    "mapping\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds_study",
   "language": "python",
   "name": "ds_study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
